{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths for the datasets\n",
    "datasets = {}\n",
    "for i in range(7, 13):\n",
    "    datasets[f'layer{i}'] = {}\n",
    "    train_path = f'./../data/layer {i}/layer_{i}_train.csv'\n",
    "    valid_path = f'./../data/layer {i}/layer_{i}_valid.csv'\n",
    "    test_path = f'./../data/layer {i}/layer_{i}_test.csv'\n",
    "    \n",
    "    datasets[f'layer{i}']['train_data'] = pd.read_csv(train_path)\n",
    "    datasets[f'layer{i}']['valid_data'] = pd.read_csv(valid_path)\n",
    "    datasets[f'layer{i}']['test_data'] = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features_layer7 = datasets['layer7']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])\n",
    "test_data_features_layer8 = datasets['layer8']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])\n",
    "test_data_features_layer9 = datasets['layer9']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])\n",
    "test_data_features_layer10 = datasets['layer10']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])\n",
    "test_data_features_layer11 = datasets['layer11']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])\n",
    "test_data_features_layer12 = datasets['layer12']['test_data'].drop(columns=['label_1', 'label_2', 'label_3', 'label_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to construct validation and training datasets for each label in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid_dataset(train_valid_dataset):\n",
    "    dataset = {}\n",
    "    \n",
    "    dataset_label1 = train_valid_dataset.drop(columns=['label_2', 'label_3', 'label_4'])\n",
    "    dataset_label2 = train_valid_dataset.drop(columns=['label_1', 'label_3', 'label_4'])\n",
    "    dataset_label3 = train_valid_dataset.drop(columns=['label_1', 'label_2', 'label_4'])\n",
    "    dataset_label4 = train_valid_dataset.drop(columns=['label_1', 'label_2', 'label_3'])\n",
    "    \n",
    "    dataset_label1 = dataset_label1.dropna(subset=dataset_label1.columns[-1:], how='any')\n",
    "    dataset_label2 = dataset_label2.dropna(subset=dataset_label2.columns[-1:], how='any')\n",
    "    dataset_label3 = dataset_label3.dropna(subset=dataset_label3.columns[-1:], how='any')\n",
    "    dataset_label4 = dataset_label4.dropna(subset=dataset_label4.columns[-1:], how='any')\n",
    "    \n",
    "    dataset_label1 = dataset_label1.fillna(dataset_label1.mean())\n",
    "    dataset_label2 = dataset_label2.fillna(dataset_label2.mean())\n",
    "    dataset_label3 = dataset_label3.fillna(dataset_label3.mean())\n",
    "    dataset_label4 = dataset_label4.fillna(dataset_label4.mean())\n",
    "    \n",
    "    dataset['label_1'] = {}\n",
    "    dataset['label_1']['features'] = dataset_label1.iloc[:, :-1]\n",
    "    dataset['label_1']['label'] = dataset_label1.iloc[:, -1]\n",
    "    \n",
    "    dataset['label_2'] = {}\n",
    "    dataset['label_2']['features'] = dataset_label2.iloc[:, :-1]\n",
    "    dataset['label_2']['label'] = dataset_label2.iloc[:, -1]\n",
    "    \n",
    "    dataset['label_3'] = {}\n",
    "    dataset['label_3']['features'] = dataset_label3.iloc[:, :-1]\n",
    "    dataset['label_3']['label'] = dataset_label3.iloc[:, -1]\n",
    "    \n",
    "    dataset['label_4'] = {}\n",
    "    dataset['label_4']['features'] = dataset_label4.iloc[:, :-1]\n",
    "    dataset['label_4']['label'] = dataset_label4.iloc[:, -1]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to construct test dataset for a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(test_dataset):    \n",
    "    test_dataset = test_dataset.fillna(test_dataset.mean())\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_datasets = {}\n",
    "for i in range(7, 13):\n",
    "    modified_datasets[f'layer{i}'] = {}\n",
    "    modified_datasets[f'layer{i}']['train_data'] = create_train_valid_dataset(datasets[f'layer{i}']['train_data'])\n",
    "    modified_datasets[f'layer{i}']['valid_data'] = create_train_valid_dataset(datasets[f'layer{i}']['valid_data'])\n",
    "    modified_datasets[f'layer{i}']['test_data'] = create_test_dataset(datasets[f'layer{i}']['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = {}\n",
    "\n",
    "for label in range(1, 5):\n",
    "    combined_dataset[f'label_{label}'] = {}\n",
    "    combined_dataset[f'label_{label}']['train'] = {}\n",
    "    combined_dataset[f'label_{label}']['valid'] = {}\n",
    "    combined_dataset[f'label_{label}']['test'] = {}\n",
    "    \n",
    "    train_features = []\n",
    "    train_label = []\n",
    "    valid_features = []\n",
    "    valid_label = []\n",
    "    test_features = []\n",
    "    for layer in range(7, 13):\n",
    "        train_features.append(modified_datasets[f'layer{layer}']['train_data'][f'label_{label}']['features'])\n",
    "        train_label.append(modified_datasets[f'layer{layer}']['train_data'][f'label_{label}']['label'])\n",
    "        valid_features.append(modified_datasets[f'layer{layer}']['valid_data'][f'label_{label}']['features'])\n",
    "        valid_label.append(modified_datasets[f'layer{layer}']['valid_data'][f'label_{label}']['label'])\n",
    "        test_features.append(modified_datasets[f'layer{layer}']['test_data'][f'label_{label}'])\n",
    "        \n",
    "    combined_dataset[f'label_{label}']['train']['features'] = pd.concat(train_features)\n",
    "    combined_dataset[f'label_{label}']['train']['label'] = pd.concat(train_label)\n",
    "    combined_dataset[f'label_{label}']['valid']['features'] = pd.concat(valid_features)\n",
    "    combined_dataset[f'label_{label}']['valid']['label'] = pd.concat(valid_label)\n",
    "    combined_dataset[f'label_{label}']['test']['features'] = pd.concat(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_components(label):\n",
    "    components = {}\n",
    "\n",
    "    for layer in range(7, 13):       \n",
    "        components[f'layer{layer}'] = {}\n",
    "        \n",
    "        if (layer == 7 and (label == 2 or label == 3)):\n",
    "            components[f'layer{layer}']['scaler'] = joblib.load(f'./../saved_components/layer{layer}/label_{label}/scaler.pkl')\n",
    "            components[f'layer{layer}']['model'] = joblib.load(f'./../saved_components/layer{layer}/label_{label}/model.pkl')\n",
    "        else:\n",
    "            components[f'layer{layer}']['scaler'] = joblib.load(f'./../saved_components/layer{layer}/label_{label}/scaler.pkl')\n",
    "            components[f'layer{layer}']['pca'] = joblib.load(f'./../saved_components/layer{layer}/label_{label}/pca.pkl')\n",
    "            components[f'layer{layer}']['model'] = joblib.load(f'./../saved_components/layer{layer}/label_{label}/model.pkl')\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to create pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipelines(components):\n",
    "    pipilines = {}\n",
    "\n",
    "    for layer in range(7, 13):\n",
    "        pipeline = []\n",
    "        if ('pca' in components[f'layer{layer}']):\n",
    "            pipilines[f'layer{layer}'] = Pipeline([\n",
    "                ('scaler', components[f'layer{layer}']['scaler']),\n",
    "                ('pca', components[f'layer{layer}']['pca']),\n",
    "                ('classifier', components[f'layer{layer}']['model'])\n",
    "            ])\n",
    "        else:\n",
    "            pipilines[f'layer{layer}'] = Pipeline([\n",
    "                ('scaler', components[f'layer{layer}']['scaler']),\n",
    "                ('classifier', components[f'layer{layer}']['model'])\n",
    "            ])\n",
    "    \n",
    "    return pipilines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to create voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble(pipelines):\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('pipeline7', pipelines['layer7']),\n",
    "        ('pipeline8', pipelines['layer8']),\n",
    "        ('pipeline9', pipelines['layer9']),\n",
    "        ('pipeline10', pipelines['layer10']),\n",
    "        ('pipeline11', pipelines['layer11']),\n",
    "        ('pipeline12', pipelines['layer12'])\n",
    "    ], voting='hard')\n",
    "    \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to test the models and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_test(model, data):\n",
    "    # Make predictions on the test data using the ensemble\n",
    "    y_valid_pred = model.predict(data['valid']['features'])\n",
    "\n",
    "    # Evaluate the ensemble on validation data\n",
    "    accuracy = accuracy_score(data['valid']['label'], y_valid_pred)\n",
    "    precision = precision_score(data['valid']['label'], y_valid_pred, average='macro', zero_division=1)\n",
    "    recall = recall_score(data['valid']['label'], y_valid_pred, average='macro')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "\n",
    "    # Predict the test data\n",
    "    y_test_pred = model.predict(data['test']['features'])\n",
    "    \n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_3_components = load_components(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_3_pipilines = make_pipelines(label_3_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer7': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('classifier', SVC(C=1000, gamma=0.001))]),\n",
       " 'layer8': Pipeline(steps=[('scaler', RobustScaler()),\n",
       "                 ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
       "                 ('classifier', SVC(C=10, class_weight='balanced'))]),\n",
       " 'layer9': Pipeline(steps=[('scaler', RobustScaler()),\n",
       "                 ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
       "                 ('classifier', SVC(C=10, gamma=0.001))]),\n",
       " 'layer10': Pipeline(steps=[('scaler', RobustScaler()),\n",
       "                 ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
       "                 ('classifier', SVC(C=10, gamma=0.001))]),\n",
       " 'layer11': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
       "                 ('classifier', SVC(C=100))]),\n",
       " 'layer12': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('pca', PCA(n_components=0.99, svd_solver='full')),\n",
       "                 ('classifier',\n",
       "                  SVC(C=100, class_weight='balanced', gamma=0.001))])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_3_pipilines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = make_ensemble(label_3_pipilines)\n",
    "\n",
    "ensemble.fit(combined_dataset['label_3']['train']['features'], combined_dataset['label_3']['train']['label'])\n",
    "\n",
    "joblib.dump(ensemble, './../saved_components/complete/label_3/ensemble.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_label_3 = validate_and_test(ensemble, combined_dataset['label_3'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
